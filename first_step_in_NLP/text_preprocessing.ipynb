{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJToNZny8wiJ"
      },
      "source": [
        "<h1><center>Предобработка текста</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIV26DDa8wiK"
      },
      "source": [
        "## Основные техники \n",
        "* Уровень символов:\n",
        "    * Токенизация: разбиение текста на слова\n",
        "    * Разбиение текста на предложения\n",
        "* Уровень слов – морфология:\n",
        "    * Разметка частей речи\n",
        "    * Снятие морфологической неоднозначности\n",
        "    * Нормализация (стемминг или лемматизация)\n",
        "* Уровень предложений – синтаксис:\n",
        "    * Выделенние именных или глагольных групп \n",
        "    * Выделенние семантических ролей\n",
        "    * Деревья составляющих и зависимостей\n",
        "* Уровень смысла – семантика и дискурс:\n",
        "    * Разрешение кореферентных связей\n",
        "    * Выделение синонимов\n",
        "    * Анализ аргументативных связей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mTIZxh28wiL"
      },
      "source": [
        "## Основные проблемы\n",
        "* Неоднозначность\n",
        "    * Лексическая неоднозначность: *орган, парить, рожки, атлас*\n",
        "    * Морфологическая неоднозначность: *Хранение денег в банке. Что делают белки в клетке?*\n",
        "    * Синтаксическая неоднозначность: *Мужу изменять нельзя. Его удивил простой солдат. Эти типы стали есть в цехе.*\n",
        "* Неологизмы: *печеньки, заинстаграммить, репостнуть, расшарить, биткоины*\n",
        "* Разные варианты написания: *Россия, Российская Федерация, РФ*\n",
        "* Нестандартное написание (в т.ч. орфографические ошибки и опечатки): *каг дила? куптиь телфон*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRVUO5KfbxaB"
      },
      "source": [
        "<img src=\"images/pipeline.png\" alt=\"pipeline.png\" style=\"width: 400px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMWEuaj8wiM"
      },
      "source": [
        "### NLP-библиотеки\n",
        "\n",
        "NLP-библиотеки для питона:\n",
        "* Natural Language Toolkit (NLTK)\n",
        "* Apache OpenNLP\n",
        "* Stanford NLP suite\n",
        "* Gate NLP library\n",
        "* Spacy\n",
        "* Yargy\n",
        "* DeepPavlov\n",
        "* CLTK (для древних языков)\n",
        "* и т.д.\n",
        "\n",
        "Самая старая и известная — NLTK. В NLTK есть не только различные инструменты для обработки текста, но и данные — текстовые корпуса, предобученные модели для анализа тональности и морфологической разметки, списки стоп-слов для разных языков и т.п.\n",
        "\n",
        "* [Учебник по NLTK](https://www.nltk.org/book/) от авторов библиотеки и [тьюториалы](https://github.com/hb20007/hands-on-nltk-tutorial) по решению разных задач NLP с помощью NLTK.\n",
        "* [Документация Spacy](https://spacy.io/)\n",
        "* [Документация Yargy](https://yargy.readthedocs.io/)\n",
        "* [Документация DeepPavlop](http://docs.deeppavlov.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJIhRR2V8wiN"
      },
      "source": [
        "## Предобработка текста\n",
        "\n",
        "1. **Токенизация** — самый первый шаг при обработке текста. \n",
        "2. **Нормализация** — приведение к одному регистру, удаляются пунктуации, исправление опечаток и т.д.\n",
        "3. \n",
        "    * **Стемминг** —  выделение псевдоосновы слова.\n",
        "    * **Лемматизация** — приведение слов к словарной (\"начальной\") форме.\n",
        "4. **Удаление стоп-слов** — слов, которые не несут никакой смысловой нагрузки (предлоги, союзы и т.п.) Список зависит от задачи!\n",
        "\n",
        "**Важно!** Не всегда нужны все этапы, все зависит от задачи!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uZhZrWq8wiN"
      },
      "source": [
        "## Токенизация\n",
        "\n",
        "#### Сколько слов в этом предложении?\n",
        "\n",
        "*На дворе трава, на траве дрова, не руби дрова на траве двора.*\n",
        "\n",
        "* 12 токенов: На, дворе, трава, на, траве, дрова, не, руби, дрова, на, траве, двора\n",
        "* 8 - 9 словоформ: Н/на, дворе, трава, траве, дрова, не, руби, двора. \n",
        "* 6  лексем: на, не, двор, трава, дрова, рубить\n",
        "\n",
        "\n",
        "### Токен и словоформа\n",
        "\n",
        "**Словоформа**  – уникальное слово из текста\n",
        "\n",
        "**Токен**  – словоформа и её позиция в тексте\n",
        "\n",
        "Объем корпуса измеряется в токенах, объем словаря — в словоформах или лексемах.\n",
        "\n",
        "### Обозначения \n",
        "$N$ = число токенов\n",
        "\n",
        "$V$ = словарь (все словоформы)\n",
        "\n",
        "$|V|$ = количество словоформ в словаре"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I2LG_Bn8wiO"
      },
      "source": [
        "### Токен ≠ слово\n",
        "\n",
        "__Рассмотрим пример:__ \n",
        "\n",
        "    Продаётся LADA 4x4. ПТС 01.12.2018, куплена 20 января 19 года, 10 000 км пробега. Комплектация полная. Новая в салоне 750 000, отдам за 650 000. Возможен обмен на ВАЗ-2110 или ВАЗ 2109 с вашей доплатой.\n",
        "\n",
        "    * Модификация: 1.6 MT (89 л.с.) \n",
        "    * Владельцев по ПТС: 4+ \n",
        "    * VIN или номер кузова: XTA21104*50****47 \n",
        "    * Мультимедиа и навигация: CD/DVD/Blu-ray \n",
        "    * Шины и диски: 14\" \n",
        "\n",
        "    Краснодар, ул. Миклухо-Маклая, д. 4/5, подъезд 1 \n",
        "\n",
        "    Тел. 8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33 \n",
        "    \n",
        "    e-mail: ivanov.ivan-61@mail.ru \n",
        "    \n",
        "    И.И. Иванов (Иван Иванович) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tMvWqXi8wiO",
        "outputId": "88b26a92-6287-4603-fada-13ca7784f034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Продаётся', 'LADA', '4x4.', 'ПТС', '01.12.2018,', 'куплена', '20', 'января', '19', 'года,', '10', '000', 'км', 'пробега.', 'Комплектация', 'полная.', 'Новая', 'в', 'салоне', '750', '000,', 'отдам', 'за', '650', '000.', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой.', 'Краснодар,', 'ул.', 'Миклухо-Маклая,', 'д.', '4/5,', 'подьезд', '1', 'Тел.', '8(999)1234567,', '8', '903', '987-65-43,', '+7', '(351)', '111', '22', '33', 'И.И.', 'Иванов', '(Иван', 'Иванович)']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# самая банальная токенизация: разбиение по пробелам\n",
        "\n",
        "text = '''\n",
        "Продаётся LADA 4x4. ПТС 01.12.2018, куплена 20 января 19 года, 10 000 км пробега. \n",
        "Комплектация полная. Новая в салоне 750 000, отдам за 650 000. \n",
        "Возможен обмен на ВАЗ-2110 или ВАЗ 2109 с вашей доплатой. \n",
        "Краснодар, ул. Миклухо-Маклая, д. 4/5, подьезд 1 \n",
        "Тел. 8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33 \n",
        "И.И. Иванов (Иван Иванович) \n",
        "'''\n",
        "\n",
        "tokens = text.split()\n",
        "print(tokens)\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s60aVHo8wiU"
      },
      "outputs": [],
      "source": [
        "# !pip install --user yargy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTXhiLHv8wiW",
        "outputId": "f5fdc8cd-9d9b-48cd-c4a4-af82b10802b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5ab3a9a31bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# максимально разбивает\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myargy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorphTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtknzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMorphTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtknzr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yargy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# максимально разбивает\n",
        "from yargy.tokenizer import MorphTokenizer\n",
        "\n",
        "tknzr = MorphTokenizer()\n",
        "tokens = [_.value for _ in tknzr(text)]\n",
        "print(tokens)\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkpDhS6h8wiZ"
      },
      "outputs": [],
      "source": [
        "# !pip install --user nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ACxgq8B8wib",
        "outputId": "c1ff8593-f352-4d1c-e143-89deac2956a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package snowball_data to /root/nltk_data...\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('snowball_data')\n",
        "nltk.download('perluniprops')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('nonbreaking_prefixes')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIEMbLSA8wig",
        "outputId": "8fd8d941-43c8-42ae-ba51-6a7b81ce51e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Продаётся', 'LADA', '4x4', '.', 'ПТС', '01.12.2018', ',', 'куплена', '20', 'января', '19', 'года', ',', '10', '000', 'км', 'пробега', '.', 'Комплектация', 'полная', '.', 'Новая', 'в', 'салоне', '750', '000', ',', 'отдам', 'за', '650', '000', '.', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой', '.', 'Краснодар', ',', 'ул', '.', 'Миклухо-Маклая', ',', 'д', '.', '4/5', ',', 'подьезд', '1', 'Тел', '.', '8', '(', '999', ')', '1234567', ',', '8', '903', '987-65-43', ',', '+7', '(', '351', ')', '111', '22', '33', 'И.И', '.', 'Иванов', '(', 'Иван', 'Иванович', ')']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxaK7tB_8wii",
        "outputId": "de94a7da-723e-4a12-c2f5-15c3ad983f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Продаётся', 'LADA', '4x4.', 'ПТС', '01.12.2018', ',', 'куплена', '20', 'января', '19', 'года', ',', '10', '000', 'км', 'пробега.', 'Комплектация', 'полная.', 'Новая', 'в', 'салоне', '750', '000', ',', 'отдам', 'за', '650', '000.', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой.', 'Краснодар', ',', 'ул.', 'Миклухо-Маклая', ',', 'д.', '4/5', ',', 'подьезд', '1', 'Тел.', '8(', '999', ')', '1234567', ',', '8', '903', '987-65-43', ',', '+7', '(', '351', ')', '111', '22', '33', 'И.И.', 'Иванов', '(', 'Иван', 'Иванович', ')']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tknzr = ToktokTokenizer()\n",
        "tokens = tknzr.tokenize(text)\n",
        "print(tokens)\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghlw0kpe8wil",
        "outputId": "219e82c3-341b-433f-d51d-f71305da9ac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@remy',\n",
              " 'This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cooool',\n",
              " '#dummysmiley',\n",
              " ':',\n",
              " ':-)',\n",
              " ':-P',\n",
              " '<3',\n",
              " 'and',\n",
              " 'some',\n",
              " 'arrows',\n",
              " '<',\n",
              " '>',\n",
              " '->',\n",
              " '<--']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# специальный токенизатор для твитов\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "tweet = \"@remy This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
        "tknzr.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5CcKw7s8win",
        "outputId": "96380fa6-f796-451b-fe7d-6b396b210c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# токенизатор на регулярных выражениях\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "s = \"Good muffins cost $3.88 in New York.  Please buy me two of them. \\n\\nThanks.\"\n",
        "tknzr = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tknzr.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5FKqaxu8wiq"
      },
      "source": [
        "В nltk вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlYxv5xy8wir",
        "outputId": "a89340b8-5550-4c53-9c0e-854d5f00e80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LegalitySyllableTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'NLTKWordTokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'SyllableTokenizer',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordDetokenizer']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ_3LRZ-8wiw"
      },
      "source": [
        "Они умеют выдавать индексы начала и конца каждого токена:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Xsd5tN8wix",
        "outputId": "920b0cbb-bf7a-478e-cf5d-06435b9711c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 5), (6, 10), (11, 13)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "list(wh_tok.span_tokenize(\"don't stop me\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VxmL3st8wiz"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbbJHhuT8wi0",
        "outputId": "ad21469e-2ddc-43c8-83d6-a0266ec355cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0qzdWqY8wi2"
      },
      "source": [
        "## Сегментация предложений\n",
        "\n",
        "Сегментацию предложений иногда называют **сплиттингом**. \n",
        "\n",
        "Основные признаки — знаки препинания. \"?\", \"!\" как правило однозначны, проблемы возникают с \".\"  Возможное решение: бинарный классификатор для сегментации предложений. Для каждой точки \".\" определить, является ли она концом предложения или нет.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX2VhQT78wi2",
        "outputId": "a8a69260-34a9-46eb-c483-1e62053162e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nПродаётся LADA 4x4.',\n",
              " 'ПТС 01.12.2018, куплена 20 января 19 года, 10 000 км пробега.',\n",
              " 'Комплектация полная.',\n",
              " 'Новая в салоне 750 000, отдам за 650 000.',\n",
              " 'Возможен обмен на ВАЗ-2110 или ВАЗ 2109 с вашей доплатой.',\n",
              " 'Краснодар, ул.',\n",
              " 'Миклухо-Маклая, д.',\n",
              " '4/5, подьезд 1 \\nТел.',\n",
              " '8(999)1234567, 8 903 987-65-43, +7 (351) 111 22 33 \\nИ.И.',\n",
              " 'Иванов (Иван Иванович)']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sents = sent_tokenize(text)\n",
        "print(len(sents))\n",
        "sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSJTZsd-8wi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4cd058-587b-4806-e72b-4e6411263a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rusenttokenize\n",
            "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: rusenttokenize\n",
            "Successfully installed rusenttokenize-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --user rusenttokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZARTC-iN8wi7",
        "outputId": "8c542893-5a7a-47ab-dea6-4a31d066c514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4275a0a5b769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrusenttokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mru_sent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mru_sent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rusenttokenize'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "sents = ru_sent_tokenize(text)\n",
        "\n",
        "print(len(sents))\n",
        "sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixKIivkm8wi-"
      },
      "source": [
        "## Нормализация\n",
        "\n",
        "### Удаление пунктуации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6E8o9W_8wi_",
        "outputId": "bef380ab-076b-484f-c12b-b8e9a9615170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Продаётся', 'LADA', '4x4.', 'ПТС', '01.12.2018,', 'куплена', '20', 'января', '19', 'года,', '10', '000', 'км', 'пробега.', 'Комплектация', 'полная.', 'Новая', 'в', 'салоне', '750', '000,', 'отдам', 'за', '650', '000.', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой.', 'Краснодар,', 'ул.', 'Миклухо-Маклая,', 'д.', '4/5,', 'подьезд', '1', 'Тел.', '8(999)1234567,', '8', '903', '987-65-43,', '+7', '(351)', '111', '22', '33', 'И.И.', 'Иванов', '(Иван', 'Иванович)']\n",
            "['Продаётся', 'LADA', '4x4', '', 'ПТС', '01.12.2018', '', 'куплена', '20', 'января', '19', 'года', '', '10', '000', 'км', 'пробега', '', 'Комплектация', 'полная', '', 'Новая', 'в', 'салоне', '750', '000', '', 'отдам', 'за', '650', '000', '', 'Возможен', 'обмен', 'на', 'ВАЗ-2110', 'или', 'ВАЗ', '2109', 'с', 'вашей', 'доплатой', '', 'Краснодар', '', 'ул', '', 'Миклухо-Маклая', '', 'д', '', '4/5', '', 'подьезд', '1', 'Тел', '', '8', '', '999', '', '1234567', '', '8', '903', '987-65-43', '', '7', '', '351', '', '111', '22', '33', 'И.И', '', 'Иванов', '', 'Иван', 'Иванович', '']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Способ №1\n",
        "import re\n",
        "\n",
        "# набор пунктуационных символов зависит от задачи и текста\n",
        "punct = '!\"#$%&()*\\+,-\\./:;<=>?@\\[\\]^_`{|}~„“«»†*\\—/\\-‘’'\n",
        "clean_text = re.sub(punct, r'', text)\n",
        "print(clean_text.split())\n",
        "\n",
        "# Способ №2\n",
        "clean_words = [w.strip(punct) for w in word_tokenize(text)]\n",
        "print(clean_words)\n",
        "\n",
        "clean_words == clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSSeJddy8wjC"
      },
      "source": [
        "### Преобразование регистра"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fyovObi8wjD",
        "outputId": "b373e4e5-1142-4e8e-e97e-028913697488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['продаётся', 'lada', '4x4', 'птс', '01.12.2018', 'куплена', '20', 'января', '19', 'года', '10', '000', 'км', 'пробега', 'комплектация', 'полная', 'новая', 'в', 'салоне', '750', '000', 'отдам', 'за', '650', '000', 'возможен', 'обмен', 'на', 'ваз-2110', 'или', 'ваз', '2109', 'с', 'вашей', 'доплатой', 'краснодар', 'ул', 'миклухо-маклая', 'д', '4/5', 'подьезд', '1', 'тел', '8', '999', '1234567', '8', '903', '987-65-43', '7', '351', '111', '22', '33', 'и.и', 'иванов', 'иван', 'иванович']\n"
          ]
        }
      ],
      "source": [
        "clean_words = [w.lower() for w in clean_words if w != '']\n",
        "print(clean_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKNDrwDQ8wjH"
      },
      "source": [
        "### Стоп-слова\n",
        "\n",
        "**Стоп-слова** — высокочастотные слова, которые не дают нам никакой информации о конкретном тексте. Они составляют верхушку частотного списка в любом языке. Набор стоп-слов не универсален, он будет зависеть от вашей задачи!\n",
        "\n",
        "В NLTK есть готовые списки стоп-слов для многих языков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42F9WthB8wjI",
        "outputId": "264ae710-228f-488f-9722-e5bfc470bcf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# смотрим, какие языки есть\n",
        "stopwords.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-GBMG498wjK",
        "outputId": "9eaca419-7d60-43bf-d434-b4ca96ad2689",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ],
      "source": [
        "sw = stopwords.words('russian')\n",
        "print(sw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY6slf1G8wjM",
        "outputId": "5a02521f-d540-40a9-9c88-a6aaa890d25c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "в\n",
            "за\n",
            "на\n",
            "или\n",
            "с\n",
            "['продаётся', 'lada', '4x4', 'птс', '01.12.2018', 'куплена', '20', 'января', '19', 'года', '10', '000', 'км', 'пробега', 'комплектация', 'полная', 'новая', None, 'салоне', '750', '000', 'отдам', None, '650', '000', 'возможен', 'обмен', None, 'ваз-2110', None, 'ваз', '2109', None, 'вашей', 'доплатой', 'краснодар', 'ул', 'миклухо-маклая', 'д', '4/5', 'подьезд', '1', 'тел', '8', '999', '1234567', '8', '903', '987-65-43', '7', '351', '111', '22', '33', 'и.и', 'иванов', 'иван', 'иванович']\n"
          ]
        }
      ],
      "source": [
        "print([w if w not in sw else print(w) for w in clean_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiMMLtZp8wjN"
      },
      "source": [
        "## Стемминг\n",
        "\n",
        "**Стемминг** — отсечение от слова окончаний и суффиксов, чтобы оставшаяся часть, называемая stem, была одинаковой для всех грамматических форм слова. Стем необязательно совпадает с морфлогической основой слова. Одинаковый стем может получиться и не у однокоренных слов и наоборот — в этом проблема стемминга. \n",
        "\n",
        "* 1-ый вид ошибки: белый, белка, белье $\\implies$  бел\n",
        "\n",
        "* 2-ой вид ошибки: трудность, трудный $\\implies$  трудност, труд \n",
        "\n",
        "* 3-ий вид ошибки: быстрый, быстрее $\\implies$  быст, побыстрее $\\implies$  побыст\n",
        "\n",
        "Самый простой алгоритм, алгоритм Портера, состоит из 5 циклов команд, на каждом цикле – операция удаления / замены суффикса. Возможны вероятностные расширения алгоритма.\n",
        "\n",
        "### Snowball stemmer\n",
        "Улучшенный вариант стеммера Портера; в отличие от него умеет работать не только с английским текстом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ni6c5T8wjO",
        "outputId": "6de75db2-d7fc-4773-9aa1-a964ed71a3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbnbMdNm8wjQ"
      },
      "outputs": [],
      "source": [
        "poem = '''\n",
        "По морям, играя, носится\n",
        "с миноносцем миноносица.\n",
        "Льнет, как будто к меду осочка,\n",
        "к миноносцу миноносочка.\n",
        "И конца б не довелось ему,\n",
        "благодушью миноносьему.\n",
        "Вдруг прожектор, вздев на нос очки,\n",
        "впился в спину миноносочки.\n",
        "Как взревет медноголосина:\n",
        "Р-р-р-астакая миноносина!\n",
        "'''\n",
        "\n",
        "words = [w.strip(punct).lower() for w in word_tokenize(poem)]\n",
        "words = [w for w in words if w not in sw and w != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrtC521I8wjW",
        "outputId": "8c8167e8-e8a4-4ff2-9b72-f73b04422deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "морям: мор\n",
            "играя: игр\n",
            "носится: нос\n",
            "миноносцем: миноносц\n",
            "миноносица: миноносиц\n",
            "льнет: льнет\n",
            "меду: мед\n",
            "осочка: осочк\n",
            "миноносцу: миноносц\n",
            "миноносочка: миноносочк\n",
            "конца: конц\n",
            "б: б\n",
            "довелось: довел\n",
            "благодушью: благодуш\n",
            "миноносьему: минонос\n",
            "прожектор: прожектор\n",
            "вздев: вздев\n",
            "нос: нос\n",
            "очки: очк\n",
            "впился: впил\n",
            "спину: спин\n",
            "миноносочки: миноносочк\n",
            "взревет: взревет\n",
            "медноголосина: медноголосин\n",
            "р-р-р-астакая: р-р-р-астак\n",
            "миноносина: миноносин\n"
          ]
        }
      ],
      "source": [
        "snowball = SnowballStemmer(\"russian\")\n",
        "\n",
        "for w in words:\n",
        "    print(\"%s: %s\" % (w, snowball.stem(w)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eREMBHVY8wjY"
      },
      "source": [
        "## Морфологический анализ\n",
        "\n",
        "Задачи морфологического анализа:\n",
        "\n",
        "* Разбор слова — определение нормальной формы (леммы), основы (стема) и грамматических характеристик слова\n",
        "* Синтез словоформы — генерация словоформы по заданным грамматическим характеристикам из леммы\n",
        "\n",
        "Морфологический анализ — не самая сильная сторона NLTK.  Для этих задач лучше использовать `pymorphy2` и `pymystem3` для русского языка и, например, `Spacy` для европейских.\n",
        "\n",
        "## Лемматизация\n",
        "\n",
        "**Лемматизация** — процесс приведения словоформы к лемме, т.е. нормальной (словарной) форме. Это более сложная задача, чем стемминг, но и результаты дает гораздо более осмысленные, особенно для языков с богатой морфологией.\n",
        "\n",
        "* кошке, кошку, кошкам, кошкой $\\implies$ кошка\n",
        "* бежал, бежит, бегу $\\implies$  бежать\n",
        "* белому, белым, белыми $\\implies$ белый\n",
        "\n",
        "## POS-tagging\n",
        "\n",
        "**Частеречная разметка**, или **POS-tagging** _(part of speech tagging)_ —  определение части речи и грамматических характеристик слов в тексте (корпусе) с приписыванием им соответствующих тегов.\n",
        "\n",
        "Для большинства слов возможно несколько разборов (т.е. несколько разных лемм, несколько разных частей речи и т.п.). Теггер генерирует  все варианты, ранжирует их по вероятности и по умолчанию выдает наиболее вероятный. Выбор одного разбора из нескольких называется **снятием омонимии**, или **дизамбигуацией**.\n",
        "\n",
        "### Наборы тегов\n",
        "\n",
        "Существует множество наборов грамматических тегов, или тегсетов:\n",
        "* НКРЯ\n",
        "* Mystem\n",
        "* UPenn\n",
        "* OpenCorpora (его использует pymorphy2)\n",
        "* Universal Dependencies\n",
        "* ...\n",
        "\n",
        "Есть даже [библиотека](https://github.com/kmike/russian-tagsets) для преобразования тегов из одной системы в другую для русского языка, `russian-tagsets`. Но важно помнить, что любое такое преобразование будет с потерями! \n",
        "\n",
        "На данный момент стандартом является **Universal Dependencies**. Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги — [вот тут](http://universaldependencies.org/u/pos/). Вот список основных (частереных) тегов UD:\n",
        "\n",
        "* ADJ: adjective\n",
        "* ADP: adposition\n",
        "* ADV: adverb\n",
        "* AUX: auxiliary\n",
        "* CCONJ: coordinating conjunction\n",
        "* DET: determiner\n",
        "* INTJ: interjection\n",
        "* NOUN: noun\n",
        "* NUM: numeral\n",
        "* PART: particle\n",
        "* PRON: pronoun\n",
        "* PROPN: proper noun\n",
        "* PUNCT: punctuation\n",
        "* SCONJ: subordinating conjunction\n",
        "* SYM: symbol\n",
        "* VERB: verb\n",
        "* X: other\n",
        "\n",
        "### pymystem3\n",
        "\n",
        "**pymystem3** — это питоновская обертка для морфологичекого анализатора Mystem, сделанного в Яндексе. Его можно скачать отдельно и использовать из консоли. Отдельный плюс Mystem - он умеет разрешать омонимию (выбирает более релевантный вариант разбора слова для данного контекста).\n",
        "\n",
        "* [Документация Mystem](https://tech.yandex.ru/mystem/doc/index-docpage/)\n",
        "* [Документация pymystem3](http://pythonhosted.org/pymystem3/)\n",
        "\n",
        "Инициализируем Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "* mystem_bin - путь к `mystem`, если их несколько\n",
        "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IClsgWM58wjY"
      },
      "outputs": [],
      "source": [
        "# ! pip install --user pymystem3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm48dgcB8wja",
        "outputId": "2c56f6d6-b8a4-46ec-a7c3-e202728ff39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['море', ' ', 'играть', ' ', 'носиться', ' ', 'миноносец', ' ', 'миноносица', ' ', 'льнуть', ' ', 'мед', ' ', 'осочка', ' ', 'миноносец', ' ', 'миноносочек', ' ', 'конец', ' ', 'б', ' ', 'доводиться', ' ', 'благодушие', ' ', 'миноносий', ' ', 'прожектор', ' ', 'вздевать', ' ', 'нос', ' ', 'очки', ' ', 'впиваться', ' ', 'спина', ' ', 'миноносочек', ' ', 'взреветь', ' ', 'медноголосина', ' ', 'р', '-', 'р', '-', 'р', '-', 'астакать', ' ', 'миноносина', '\\n']\n"
          ]
        }
      ],
      "source": [
        "from pymystem3 import Mystem\n",
        "\n",
        "m = Mystem()\n",
        "lemmas = m.lemmatize(' '.join(words))\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04rPVaGg8wjd",
        "outputId": "5122f269-d85f-45b8-bcd3-b3b69b081ba0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': '\\n'},\n",
              " {'analysis': [{'lex': 'по', 'wt': 1, 'gr': 'PR='}], 'text': 'По'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'море', 'wt': 1, 'gr': 'S,сред,неод=дат,мн'}],\n",
              "  'text': 'морям'},\n",
              " {'text': ', '},\n",
              " {'analysis': [{'lex': 'играть', 'wt': 1, 'gr': 'V,несов,пе=непрош,деепр'}],\n",
              "  'text': 'играя'},\n",
              " {'text': ', '},\n",
              " {'analysis': [{'lex': 'носиться',\n",
              "    'wt': 1,\n",
              "    'gr': 'V,несов,нп=непрош,ед,изъяв,3-л'}],\n",
              "  'text': 'носится'},\n",
              " {'text': '\\n'},\n",
              " {'analysis': [{'lex': 'с', 'wt': 0.999977831, 'gr': 'PR='}], 'text': 'с'}]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed = m.analyze(poem)\n",
        "parsed[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMf3iMoe8wjf",
        "outputId": "67a71938-9b6a-4888-c167-32cb2c1a52a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "По PR\n",
            "морям S\n",
            "играя V\n",
            "носится V\n",
            "с PR\n",
            "миноносцем S\n",
            "миноносица S\n",
            "Льнет V\n",
            "как ADVPRO\n"
          ]
        }
      ],
      "source": [
        "# как достать части речи\n",
        "\n",
        "for word in parsed[:20]:\n",
        "    if 'analysis' in word:\n",
        "        gr = word['analysis'][0]['gr']\n",
        "        pos = gr.split('=')[0].split(',')[0]\n",
        "        print(word['text'], pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTJsMuTX8wjh"
      },
      "source": [
        "###  pymorphy2\n",
        "\n",
        "**pymorphy2** — это полноценный морфологический анализатор, целиком написанный на Python. В отличие от Mystem, он не учитывает контекст, а значит, вопрос разрешения омонимии надо будет решать нам самим (об этом ниже). Он также умеет ставить слова в нужную форму (спрягать и склонять). \n",
        "\n",
        "[Документация pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fGOCBpL8wji"
      },
      "outputs": [],
      "source": [
        "# ! pip install --user pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxFHvLVx8wjm",
        "outputId": "4ff64967-8700-4690-b8e2-ebe62d12a418"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
              " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
              " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
              " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
              " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
              " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "p = morph.parse('стали')\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGfkuMnT8wjo",
        "outputId": "f8b55225-f9d7-48bf-e62b-87ea03b3dedc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слово: стали\n",
            "Тэг: VERB,perf,intr plur,past,indc\n",
            "Лемма: стать\n",
            "Вероятность: 0.984662\n"
          ]
        }
      ],
      "source": [
        "first = p[0]  # первый разбор\n",
        "print('Слово:', first.word)\n",
        "print('Тэг:', first.tag)\n",
        "print('Лемма:', first.normal_form)\n",
        "print('Вероятность:', first.score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_vXOl2G8wjq"
      },
      "source": [
        "Из каждого тега можно достать более дробную информацию. Если граммема есть в разборе, то вернется ее значение, если ее нет, то вернется None. [Список граммем](https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfN8Jlav8wjq",
        "outputId": "25c17742-4cf6-4f94-c7a7-ad0547677d92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse(word='стать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='стать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стать', 904, 0),))\n",
            "VERB\n",
            "None\n",
            "perf\n",
            "None\n",
            "None\n",
            "None\n",
            "indc\n",
            "plur\n",
            "None\n",
            "past\n",
            "intr\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(first.normalized)        # лемма\n",
        "print(first.tag.POS)           # Part of Speech, часть речи\n",
        "print(first.tag.animacy)       # одушевленность\n",
        "print(first.tag.aspect)        # вид: совершенный или несовершенный\n",
        "print(first.tag.case)          # падеж\n",
        "print(first.tag.gender)        # род (мужской, женский, средний)\n",
        "print(first.tag.involvement)   # включенность говорящего в действие\n",
        "print(first.tag.mood)          # наклонение (повелительное, изъявительное)\n",
        "print(first.tag.number)        # число (единственное, множественное)\n",
        "print(first.tag.person)        # лицо (1, 2, 3)\n",
        "print(first.tag.tense)         # время (настоящее, прошедшее, будущее)\n",
        "print(first.tag.transitivity)  # переходность (переходный, непереходный)\n",
        "print(first.tag.voice)         # залог (действительный, страдательный)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZtVGFG08wjt",
        "outputId": "c28492a8-f725-42c1-bfa3-f69c361600b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse(word='стать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='стать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стать', 904, 0),))\n",
            "VERB\n",
            "perf\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(first.normalized)      \n",
        "print(first.tag.POS)\n",
        "print(first.tag.aspect)\n",
        "print(first.tag.case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZA9awaI8wjy"
      },
      "source": [
        "### mystem vs. pymorphy\n",
        "\n",
        "1) Оба они могут работать с незнакомыми словами (out-of-vocabulary words, OOV).\n",
        "\n",
        "2) *Скорость*. Mystem работает невероятно медленно под windows на больших текстах, но очень быстро, елси запускать из консоли в linux / mac os.\n",
        "\n",
        "3) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot_d8SfS8wjz",
        "outputId": "193e14ea-4c62-4554-9a8b-dfc6633e0348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
            "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
          ]
        }
      ],
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
        "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-jgHkHi8wj2"
      },
      "outputs": [],
      "source": [
        "p = morph.parse('сорока')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp_VxYoJ8wj4",
        "outputId": "08219ec5-3143-4862-e6cd-eb979d9a4223"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parse(word='сорока', tag=OpencorporaTag('NUMR loct'), normal_form='сорок', score=0.285714, methods_stack=((<DictionaryAnalyzer>, 'сорока', 2802, 5),)),\n",
              " Parse(word='сорока', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='сорока', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'сорока', 43, 0),)),\n",
              " Parse(word='сорока', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='сорока', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'сорока', 403, 0),)),\n",
              " Parse(word='сорока', tag=OpencorporaTag('NUMR gent'), normal_form='сорок', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'сорока', 2802, 1),)),\n",
              " Parse(word='сорока', tag=OpencorporaTag('NUMR datv'), normal_form='сорок', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'сорока', 2802, 2),)),\n",
              " Parse(word='сорока', tag=OpencorporaTag('NUMR ablt'), normal_form='сорок', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'сорока', 2802, 4),))]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f54Z_mabxaP"
      },
      "source": [
        "### Собираем все вместе:\n",
        "\n",
        "Сделаем стандартную предобработку данных с сайта Lenta.ru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78QDJ8qGbxaP",
        "outputId": "44b9f72d-b65d-46ca-ad2b-76196bb860bd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8930</th>\n",
              "      <td>Американский миллиардер Уоррен Баффет заявил, что больше не будет предлагать трем крупнейшим страховщикам облигаций перестраховать 800 миллиардов долларов в муниципальных облигациях. Об этом сообщает агентство Reuters. Ранее подобное предложение получили MBIA, Ambac Financial Group и FGIC. В начале февраля 2008 года предложение Уоррена Баффета было воспринято инвесторами с воодушевлением, поскольку они опасались того, что ситуация со страховщиками лишь усугубит положение на кредитных рынках. Однако позже одна из трех страховых компаний от предложения Баффета отказалась, а две другие не дали ответа. Причиной стало то, что перестрахование может стоить на 50 процентов больше, чем компании смогут получить по страховой премии. Ожидаемые убытки, связанные с облигациями, вынудили страховые ко...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4633</th>\n",
              "      <td>ВВС Украины приняли на вооружение российские модернизированные штурмовики Су-25, говорится в пресс-релизе Министерства обороны Украины. Точное количество принятых на вооружение самолетов не уточняется, однако сообщается, что речь идет об учебно-боевых самолетах Су-25УБМ1 и боевых Су-25М1. По сведениям Министерства обороны Украины, Су-25УБМ1 предназначен для подготовки пилотов в боевых подразделениях. Этот самолет представляет собой модернизированную версию Су-25УБ. В частности, на самолете установлено новое оборудование, которое позволяет повысить точность применения бомб, ракет и пушечного вооружения Су-25УБМ1. Подробности о том, какое именно оборудование было установлено на модернизированные Су-25УБ не приводятся. Тем не менее, известно, что аналогичная аппаратура была установлена и ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3581</th>\n",
              "      <td>Международная ассоциация легкоатлетических федераций (IAAF) подала апелляцию на решение Российского антидопингового агентства (РУСАДА) не лишать ходоков Сергея Кирдяпкина и Ольгу Каниськину золотой и серебряной медалей Олимпиады в Лондоне соответственно. Об этом сообщается на официальном сайте РУСАДА. IAAF оспорила вердикт российской организации в Спортивном арбитражном суде. Согласно решению РУСАДА, Кирдяпкин и Каниськина были дисквалифицированы на три года и два месяца каждый за употребление допинга, но при этом не лишены олимпийских наград. Российское антидопинговое агентство 20 января объявило об отстранении от соревнований на разные сроки и об аннулировании результатов за отдельные периоды с 2009 по 2012 год нескольких ходоков — олимпийских чемпионов Каниськиной, Кирдяпкина, Валер...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7589</th>\n",
              "      <td>Арбитражный суд Башкирии отказал АФК «Система» и «Системе-Инвест» в отмене принятых по просьбе «Роснефти» обеспечительных мер в виде ареста принадлежащих АФК акций трех компаний балансовой стоимостью 185 миллиардов рублей. Об этом сообщает РИА новости. «В удовлетворении заявления АФК \"Система\" и АО \"Система-Инвест\" об отмене обеспечительных мер отказать», — заявила судья Ирина Нурисламова, оглашая определение суда. «Доводы АФК \"Система\" о том, что «действительная стоимость активов \"Системы\" (1,104 миллиарда рублей) более чем 6,5 раза превышает сумму заявленных к ней требований, чего более чем достаточно для исполнения решения суда», основаны на отчетности по МСФО, которая содержит информацию об активах всей группы АФК, включая дочерние и зависимые компании, которые не являются ответчик...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1757</th>\n",
              "      <td>Семейный скандал в гостиничном номере Лас-Вегаса привел к тому, что одна его участница - телепродюсер Райан Хэддон - оказалась на 24 часа за решеткой, а другой - киноактер Кристиан Слейтер (Christian Slater) - в больнице, где ему на голову пришлось наложить 20 швов. Предполагается, пишет msnbc.com, что во время потасовки в номере Hard Rock hotel-casino  Хэддон разбила о его голову стакан. Других подробностей агентство не сообщает. 34-летний Слейтер известен по таким фильмам-экшн, как \"Сломанная стрела\" и \"3000 миль до Грейсленда\", \"Робин Гуд - Принц воров\" и др. Супруги женаты с февраля 2000 года. Это второй брак актера.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text\n",
              "8930  Американский миллиардер Уоррен Баффет заявил, что больше не будет предлагать трем крупнейшим страховщикам облигаций перестраховать 800 миллиардов долларов в муниципальных облигациях. Об этом сообщает агентство Reuters. Ранее подобное предложение получили MBIA, Ambac Financial Group и FGIC. В начале февраля 2008 года предложение Уоррена Баффета было воспринято инвесторами с воодушевлением, поскольку они опасались того, что ситуация со страховщиками лишь усугубит положение на кредитных рынках. Однако позже одна из трех страховых компаний от предложения Баффета отказалась, а две другие не дали ответа. Причиной стало то, что перестрахование может стоить на 50 процентов больше, чем компании смогут получить по страховой премии. Ожидаемые убытки, связанные с облигациями, вынудили страховые ко...\n",
              "4633  ВВС Украины приняли на вооружение российские модернизированные штурмовики Су-25, говорится в пресс-релизе Министерства обороны Украины. Точное количество принятых на вооружение самолетов не уточняется, однако сообщается, что речь идет об учебно-боевых самолетах Су-25УБМ1 и боевых Су-25М1. По сведениям Министерства обороны Украины, Су-25УБМ1 предназначен для подготовки пилотов в боевых подразделениях. Этот самолет представляет собой модернизированную версию Су-25УБ. В частности, на самолете установлено новое оборудование, которое позволяет повысить точность применения бомб, ракет и пушечного вооружения Су-25УБМ1. Подробности о том, какое именно оборудование было установлено на модернизированные Су-25УБ не приводятся. Тем не менее, известно, что аналогичная аппаратура была установлена и ...\n",
              "3581  Международная ассоциация легкоатлетических федераций (IAAF) подала апелляцию на решение Российского антидопингового агентства (РУСАДА) не лишать ходоков Сергея Кирдяпкина и Ольгу Каниськину золотой и серебряной медалей Олимпиады в Лондоне соответственно. Об этом сообщается на официальном сайте РУСАДА. IAAF оспорила вердикт российской организации в Спортивном арбитражном суде. Согласно решению РУСАДА, Кирдяпкин и Каниськина были дисквалифицированы на три года и два месяца каждый за употребление допинга, но при этом не лишены олимпийских наград. Российское антидопинговое агентство 20 января объявило об отстранении от соревнований на разные сроки и об аннулировании результатов за отдельные периоды с 2009 по 2012 год нескольких ходоков — олимпийских чемпионов Каниськиной, Кирдяпкина, Валер...\n",
              "7589  Арбитражный суд Башкирии отказал АФК «Система» и «Системе-Инвест» в отмене принятых по просьбе «Роснефти» обеспечительных мер в виде ареста принадлежащих АФК акций трех компаний балансовой стоимостью 185 миллиардов рублей. Об этом сообщает РИА новости. «В удовлетворении заявления АФК \"Система\" и АО \"Система-Инвест\" об отмене обеспечительных мер отказать», — заявила судья Ирина Нурисламова, оглашая определение суда. «Доводы АФК \"Система\" о том, что «действительная стоимость активов \"Системы\" (1,104 миллиарда рублей) более чем 6,5 раза превышает сумму заявленных к ней требований, чего более чем достаточно для исполнения решения суда», основаны на отчетности по МСФО, которая содержит информацию об активах всей группы АФК, включая дочерние и зависимые компании, которые не являются ответчик...\n",
              "1757                                                                                                                                                                             Семейный скандал в гостиничном номере Лас-Вегаса привел к тому, что одна его участница - телепродюсер Райан Хэддон - оказалась на 24 часа за решеткой, а другой - киноактер Кристиан Слейтер (Christian Slater) - в больнице, где ему на голову пришлось наложить 20 швов. Предполагается, пишет msnbc.com, что во время потасовки в номере Hard Rock hotel-casino  Хэддон разбила о его голову стакан. Других подробностей агентство не сообщает. 34-летний Слейтер известен по таким фильмам-экшн, как \"Сломанная стрела\" и \"3000 миль до Грейсленда\", \"Робин Гуд - Принц воров\" и др. Супруги женаты с февраля 2000 года. Это второй брак актера."
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)  \n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('max_colwidth', 800)\n",
        "\n",
        "data = pd.read_csv('./data/lenta-ru-partial.csv', usecols=['text'])\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30iRgGBUbxaQ"
      },
      "outputs": [],
      "source": [
        "m = MorphAnalyzer()\n",
        "\n",
        "# убираем все небуквенные символы\n",
        "regex = re.compile(\"[А-Яа-яA-z]+\")\n",
        "\n",
        "def words_only(text, regex=regex):\n",
        "    try:\n",
        "        return regex.findall(text.lower())\n",
        "    except:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDxKvrXJbxaQ",
        "outputId": "42b8c1d3-d462-46d0-d339-ffb8e2539194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "В южноафриканском Кейптауне победой сборной России завершился чемпионат мира среди бездомных. В финальном матче российские футболисты, впервые в своей истории ставшие чемпионами мира, обыграли команду Казахстана со счетом 1:0, передает BBC News. В первенстве принимали участие почти 500 человек, которые представляли 48 стран мира. Все матчи, каждый из которых продолжался 15 минут, проходили на асфальтовых полях, причем в одной команде могли играть как мужчины, так и женщины. Сборная России провела на турнире 13 матчей, во всех из которых добилась победы. На предыдущих чемпионатах мира достижения российской команды были скромнее: в 2003-м году – 13-е место, в 2004-м году – 5-е место, в 2005-м году – 12-е место.\n"
          ]
        }
      ],
      "source": [
        "print(data.text[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DoFQEU1bxaQ",
        "outputId": "a2fd03b7-4f94-4cfa-bd86-4fff98bce815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "в южноафриканском кейптауне победой сборной россии завершился чемпионат мира среди бездомных в финальном матче российские футболисты впервые в своей истории ставшие чемпионами мира обыграли команду казахстана со счетом передает bbc news в первенстве принимали участие почти человек которые представляли стран мира все матчи каждый из которых продолжался минут проходили на асфальтовых полях причем в одной команде могли играть как мужчины так и женщины сборная россии провела на турнире матчей во всех из которых добилась победы на предыдущих чемпионатах мира достижения российской команды были скромнее в м году е место в м году е место в м году е место\n"
          ]
        }
      ],
      "source": [
        "print(*words_only(data.text[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpqQV2cXbxaQ"
      },
      "source": [
        "Метод @lru_cashe создает для функции lemmatize кэш указанного размера, что позволяет в целом ускорить лемматизацию текста (что очень полезно, так как лемматизация - ресурсоемкий процесс)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sGUafjTbxaQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lemmatize_word(token, pymorphy=m):\n",
        "    return pymorphy.parse(token)[0].normal_form\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatize_word(w) for w in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkSVu1okbxaQ",
        "outputId": "5f7d0499-df4e-4a3a-def6-9cc97f77cee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['в', 'южноафриканский', 'кейптаун', 'победа', 'сборный', 'россия', 'завершиться', 'чемпионат', 'мир', 'среди', 'бездомный', 'в', 'финальный', 'матч', 'российский', 'футболист', 'впервые', 'в', 'свой', 'история', 'стать', 'чемпион', 'мир', 'обыграть', 'команда', 'казахстан', 'с', 'счёт', 'передавать', 'bbc', 'news', 'в', 'первенство', 'принимать', 'участие', 'почти', 'человек', 'который', 'представлять', 'страна', 'мир', 'весь', 'матч', 'каждый', 'из', 'который', 'продолжаться', 'минута', 'проходить', 'на', 'асфальтовый', 'поле', 'причём', 'в', 'один', 'команда', 'мочь', 'играть', 'как', 'мужчина', 'так', 'и', 'женщина', 'сборная', 'россия', 'провести', 'на', 'турнир', 'матч', 'в', 'весь', 'из', 'который', 'добиться', 'победа', 'на', 'предыдущий', 'чемпионат', 'мир', 'достижение', 'российский', 'команда', 'быть', 'скромный', 'в', 'метр', 'год', 'е', 'место', 'в', 'метр', 'год', 'е', 'место', 'в', 'метр', 'год', 'е', 'место']\n"
          ]
        }
      ],
      "source": [
        "tokens = words_only(data.text[0])\n",
        "\n",
        "print(lemmatize_text(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAfP8610bxaR"
      },
      "outputs": [],
      "source": [
        "mystopwords = stopwords.words('russian') \n",
        "\n",
        "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
        "    return [w for w in lemmas if not w in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmoZhyLnbxaR",
        "outputId": "fe80dd71-e845-4a4a-cd88-137f7c2379e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "южноафриканский кейптаун победа сборный россия завершиться чемпионат мир среди бездомный финальный матч российский футболист впервые свой история стать чемпион мир обыграть команда казахстан счёт передавать bbc news первенство принимать участие человек который представлять страна мир весь матч каждый который продолжаться минута проходить асфальтовый поле причём команда мочь играть мужчина женщина сборная россия провести турнир матч весь который добиться победа предыдущий чемпионат мир достижение российский команда скромный метр год е место метр год е место метр год е место\n"
          ]
        }
      ],
      "source": [
        "lemmas = lemmatize_text(tokens)\n",
        "\n",
        "print(*remove_stopwords(lemmas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAuIEQ7BbxaR"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
        "    return [w for w in lemmas if not w in stopwords and len(w) > 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5m4jqKbxaR",
        "outputId": "f9964cdb-20f3-45ff-978f-d200afc8cd9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "южноафриканский кейптаун победа сборный россия завершиться чемпионат среди бездомный финальный матч российский футболист впервые свой история стать чемпион обыграть команда казахстан счёт передавать news первенство принимать участие человек который представлять страна весь матч каждый который продолжаться минута проходить асфальтовый поле причём команда мочь играть мужчина женщина сборная россия провести турнир матч весь который добиться победа предыдущий чемпионат достижение российский команда скромный метр место метр место метр место\n"
          ]
        }
      ],
      "source": [
        "print(*remove_stopwords(lemmas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tawlOV_bbxaR"
      },
      "source": [
        "Если собрать все в одну функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQiXr5H7bxaR"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    tokens = words_only(text)\n",
        "    lemmas = lemmatize_text(tokens)\n",
        "    \n",
        "    return remove_stopwords(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-HTDsQFbxaR",
        "outputId": "85d7b2cf-62e1-4899-b411-5d4f5d2719d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "известный голливудский актёр майкл дуглас совершить неожиданный визит сообщать издание cubadebate цель поездка дуглас уточняться утверждаться лишь актёр посетить несколько памятный место число закусочный floridita который некогда любить бывать эрнест хемингуэй майкл дуглас также осмотреть достопримечательность исторический центр гавана понаблюдать процесс изготовление кубинский сигара табачный фабрика стоить отметить свободный посещение куба американский гражданин иметь родственник остров запретить американец поездка требоваться специальный разрешение государственный департамент получать дуглас разрешение сообщаться напомнить дуглас единственный знаменитый голливудский актёр посетить последний время ранее страна качество корреспондент журнал vanity fair прибыть визит пенна двукратный обладатель премия оскар намереваться взять интервью фидель кастро поездка пенный сопровождать известный филантроп дайана дженкинс\n"
          ]
        }
      ],
      "source": [
        "print(*clean_text(data.text[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG8MmG4TbxaS"
      },
      "source": [
        "Если нужно предобработать большой объем текста, помимо кэширования может помочь распараллеливание, например, методом Pool библиотеки multiprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfzEpdeobxaS",
        "outputId": "984d2528-c1a0-4bd7-c61d-d5afa800450b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  import sys\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b23437189cab4859801cf5d76490dcc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "N = 200\n",
        "\n",
        "with Pool(4) as p:\n",
        "    lemmas = list(tqdm(p.imap(clean_text, data['text'][:N]), total=N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MUkN2KjbxaS",
        "outputId": "dc46580b-e1ac-4c46-e413-1e05c8d7f092"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Предприятия, работающие в арктическом регионе, дают 10 процентов ВВП России. Об этом в четверг, 30 марта, заявил президент Владимир Путин на форуме «Арктика — территория диалога», передает Rambler News Service. «Вряд ли что-то может поменять наши приоритеты в этом регионе. Имею в виду несколько обстоятельств. Первое: уже сегодня 10 процентов ВВП России складывается из результатов работы предприятий, функционирующих в этом регионе. Удельный вес их постоянно растет», — приводит агентство слова главы государства. Президент также назвал еще один фактор, влияющий на развитие арктического региона. «Существенным является следующее обстоятельство: это рост, изменение и повышение эффективности новейших технологий, которые появляются», — добавил президент. Объем экономики по итогам 2016 года сос...</td>\n",
              "      <td>[предприятие, работать, арктический, регион, давать, процент, россия, четверг, март, заявить, президент, владимир, путин, форум, арктика, территория, диалог, передавать, rambler, news, service, вряд, мочь, поменять, приоритет, регион, иметь, несколько, обстоятельство, первое, сегодня, процент, россия, складываться, результат, работа, предприятие, функционировать, регион, удельный, постоянно, расти, приводить, агентство, слово, глава, государство, президент, также, назвать, фактор, влиять, развитие, арктический, регион, существенный, являться, следующий, обстоятельство, рост, изменение, повышение, эффективность, новый, технология, который, появляться, добавить, президент, объесть, экономика, итог, составить, триллион, рубль, согласно, базовый, прогноз, минэкономразвития, увеличиться, пр...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Букмекеры оценили шансы российского биатлониста Антона Шипулина на победу в Кубке мира. Об этом в субботу, 26 ноября, сообщает «Чемпионат.com». На первое место россиянина можно поставить с коэффициентом 11. В то же время главный фаворит соревнований — француз Мартен Фуркад. Вероятность победы спортсмена оценивается коэффициентом 1,30. Шансы норвежца Йоханнеса Бе на «Хрустальный глобус» равны 7,5. Третьим претендентом на титул букмекеры назвали немецкого биатлониста Симона Шемппа. На его победу можно поставить с коэффициентом 8,4. Шансы еще одного россиянина Евгения Гараничева оценены в 71, а его товарища по команде Дмитрия Малышко — в 301. В сезоне 2015/2016 обладателем «Хрустального глобуса» стал Фуркад. Второе место в общем зачете занял Бе, третьим стал Шипулин. Первый этап Кубка мир...</td>\n",
              "      <td>[букмекер, оценить, шанс, российский, биатлонист, антон, шипулина, победа, кубок, суббота, ноябрь, сообщать, чемпионат, первое, место, россиянин, поставить, коэффициент, время, главный, фаворит, соревнование, француз, мартен, фуркад, вероятность, победа, спортсмен, оцениваться, коэффициент, шанс, норвежец, йоханнес, хрустальный, глобус, равный, претендент, титул, букмекер, назвать, немецкий, биатлонист, симон, шемппа, победа, поставить, коэффициент, шанс, россиянин, евгений, гараничев, оценить, товарищ, команда, дмитрия, малышко, сезон, обладатель, хрустальный, глобус, стать, фуркад, место, общий, зачёт, занять, стать, шипулина, этап, кубок, биатлон, сезон, стартовать, воскресение, ноябрь, шведский, эстерсунд, программа, соревнование, войти, смешанный, эстафета, супермикст, индивидуаль...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>Неполное страховое покрытие вкладов с высокой доходностью может снизить устойчивость российской банковской системы. К такому выводу пришли эксперты в бюллетене Центра развития Высшей школы экономики (ВШЭ). К числу негативных последствий они также отнесли снижение конкуренции между банками. На минувшей неделе премьер-министр России Дмитрий Медведев поддержал увеличение государственной страховки по вкладам до миллиона рублей. В то же время на страховки по вкладам было предложено наложить ряд ограничений. В частности, ЦБ не исключает введения неполной страховки по высокодоходным вкладам. По мнению экспертов ВШЭ, увеличение максимального страхового покрытия может привести к некоторому перетоку средств населения в те банки, которые предлагают клиентам более выгодные условия обслуживания. \"П...</td>\n",
              "      <td>[неполный, страховой, покрытие, вклад, высокий, доходность, мочь, снизить, устойчивость, российский, банковский, система, вывод, прислать, эксперт, бюллетень, центр, развитие, высокий, школа, экономика, число, негативный, последствие, также, отнести, снижение, конкуренция, банка, минувший, неделя, премьер, министр, россия, дмитрий, медведев, поддержать, увеличение, государственный, страховка, вклад, миллион, рубль, время, страховка, вклад, предложить, наложить, ограничение, частность, исключать, введение, неполный, страховка, высокодоходный, вклад, мнение, эксперт, увеличение, максимальный, страховой, покрытие, мочь, привести, некоторый, переток, средство, население, банка, который, предлагать, клиент, выгодный, условие, обслуживание, поэтому, неудивительно, госбанк, привыкнуть, жить, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           lemmas\n",
              "19   Предприятия, работающие в арктическом регионе, дают 10 процентов ВВП России. Об этом в четверг, 30 марта, заявил президент Владимир Путин на форуме «Арктика — территория диалога», передает Rambler News Service. «Вряд ли что-то может поменять наши приоритеты в этом регионе. Имею в виду несколько обстоятельств. Первое: уже сегодня 10 процентов ВВП России складывается из результатов работы предприятий, функционирующих в этом регионе. Удельный вес их постоянно растет», — приводит агентство слова главы государства. Президент также назвал еще один фактор, влияющий на развитие арктического региона. «Существенным является следующее обстоятельство: это рост, изменение и повышение эффективности новейших технологий, которые появляются», — добавил президент. Объем экономики по итогам 2016 года сос...  [предприятие, работать, арктический, регион, давать, процент, россия, четверг, март, заявить, президент, владимир, путин, форум, арктика, территория, диалог, передавать, rambler, news, service, вряд, мочь, поменять, приоритет, регион, иметь, несколько, обстоятельство, первое, сегодня, процент, россия, складываться, результат, работа, предприятие, функционировать, регион, удельный, постоянно, расти, приводить, агентство, слово, глава, государство, президент, также, назвать, фактор, влиять, развитие, арктический, регион, существенный, являться, следующий, обстоятельство, рост, изменение, повышение, эффективность, новый, технология, который, появляться, добавить, президент, объесть, экономика, итог, составить, триллион, рубль, согласно, базовый, прогноз, минэкономразвития, увеличиться, пр...\n",
              "7    Букмекеры оценили шансы российского биатлониста Антона Шипулина на победу в Кубке мира. Об этом в субботу, 26 ноября, сообщает «Чемпионат.com». На первое место россиянина можно поставить с коэффициентом 11. В то же время главный фаворит соревнований — француз Мартен Фуркад. Вероятность победы спортсмена оценивается коэффициентом 1,30. Шансы норвежца Йоханнеса Бе на «Хрустальный глобус» равны 7,5. Третьим претендентом на титул букмекеры назвали немецкого биатлониста Симона Шемппа. На его победу можно поставить с коэффициентом 8,4. Шансы еще одного россиянина Евгения Гараничева оценены в 71, а его товарища по команде Дмитрия Малышко — в 301. В сезоне 2015/2016 обладателем «Хрустального глобуса» стал Фуркад. Второе место в общем зачете занял Бе, третьим стал Шипулин. Первый этап Кубка мир...  [букмекер, оценить, шанс, российский, биатлонист, антон, шипулина, победа, кубок, суббота, ноябрь, сообщать, чемпионат, первое, место, россиянин, поставить, коэффициент, время, главный, фаворит, соревнование, француз, мартен, фуркад, вероятность, победа, спортсмен, оцениваться, коэффициент, шанс, норвежец, йоханнес, хрустальный, глобус, равный, претендент, титул, букмекер, назвать, немецкий, биатлонист, симон, шемппа, победа, поставить, коэффициент, шанс, россиянин, евгений, гараничев, оценить, товарищ, команда, дмитрия, малышко, сезон, обладатель, хрустальный, глобус, стать, фуркад, место, общий, зачёт, занять, стать, шипулина, этап, кубок, биатлон, сезон, стартовать, воскресение, ноябрь, шведский, эстерсунд, программа, соревнование, войти, смешанный, эстафета, супермикст, индивидуаль...\n",
              "155  Неполное страховое покрытие вкладов с высокой доходностью может снизить устойчивость российской банковской системы. К такому выводу пришли эксперты в бюллетене Центра развития Высшей школы экономики (ВШЭ). К числу негативных последствий они также отнесли снижение конкуренции между банками. На минувшей неделе премьер-министр России Дмитрий Медведев поддержал увеличение государственной страховки по вкладам до миллиона рублей. В то же время на страховки по вкладам было предложено наложить ряд ограничений. В частности, ЦБ не исключает введения неполной страховки по высокодоходным вкладам. По мнению экспертов ВШЭ, увеличение максимального страхового покрытия может привести к некоторому перетоку средств населения в те банки, которые предлагают клиентам более выгодные условия обслуживания. \"П...  [неполный, страховой, покрытие, вклад, высокий, доходность, мочь, снизить, устойчивость, российский, банковский, система, вывод, прислать, эксперт, бюллетень, центр, развитие, высокий, школа, экономика, число, негативный, последствие, также, отнести, снижение, конкуренция, банка, минувший, неделя, премьер, министр, россия, дмитрий, медведев, поддержать, увеличение, государственный, страховка, вклад, миллион, рубль, время, страховка, вклад, предложить, наложить, ограничение, частность, исключать, введение, неполный, страховка, высокодоходный, вклад, мнение, эксперт, увеличение, максимальный, страховой, покрытие, мочь, привести, некоторый, переток, средство, население, банка, который, предлагать, клиент, выгодный, условие, обслуживание, поэтому, неудивительно, госбанк, привыкнуть, жить, ..."
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data.head(200)\n",
        "data['lemmas'] = lemmas\n",
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nADMfeJNbxaS"
      },
      "source": [
        "### Итого:\n",
        "\n",
        "- посмотрели, как делать все стандартные этапы предобработки текста\n",
        "- научились работать с морфологоческими парсерами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wTjLOlc8ib0"
      },
      "source": [
        "<h1><center>Простые векторные модели текста</center></h1>\n",
        "\n",
        "<img src=\"images/pipeline_vec.png\" alt=\"pipeline_vec.png\" style=\"width: 400px;\"/>\n",
        "\n",
        "### Задача: классификация твитов по тональности\n",
        "\n",
        "В этом занятии мы познакомимся с распространенной задачей в анализе текстов: с классификацией текстов на классы.\n",
        "\n",
        "В рассмотренном тут примере классов будет два: положительный и отрицательный, такую постановку этой задачи обычно называют классификацией по тональности или sentiment analysis.\n",
        "\n",
        "Классификацию по тональности используют, например, в рекомендательных системах и при анализе отзывов клиентов, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
        "\n",
        "Более подробно мы рассмотрим данную задачу и познакомимся с более сложными методами её решения в семинаре 3, а здесь разберем простые подходы, основанные на методе мешка слов.\n",
        "\n",
        "У нас есть [данные постов в твиттере](http://study.mokoron.com/), про из которых каждый указано, как он эмоционально окрашен: положительно или отрицательно. \n",
        "\n",
        "**Задача**: построить модель, которая по тексту поста предсказывает его эмоциональную окраску.\n",
        "\n",
        "\n",
        "Скачиваем данные: [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuDVGp4O8ib1",
        "outputId": "a37fe485-de8a-4178-f7f1-03e41a83c081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-03 09:09:30--  https://raw.githubusercontent.com/MentatRus/twitter-sentiment/master/positive.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/plain]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  25.02M  99.0MB/s    in 0.3s    \n",
            "\n",
            "2022-12-03 09:09:32 (99.0 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
            "\n",
            "--2022-12-03 09:09:33--  https://raw.githubusercontent.com/MentatRus/twitter-sentiment/master/negative.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/plain]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  23.32M   127MB/s    in 0.2s    \n",
            "\n",
            "2022-12-03 09:09:35 (127 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
        "!wget https://raw.githubusercontent.com/MentatRus/twitter-sentiment/master/positive.csv\n",
        "!wget  https://raw.githubusercontent.com/MentatRus/twitter-sentiment/master/negative.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPp8_2Sy8ib5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pd.set_option('display.max_columns', None)  \n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('max_colwidth', 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsmSQOE98ib8"
      },
      "outputs": [],
      "source": [
        "positive = pd.read_csv('./positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('./negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znc9rKWk8ib-",
        "outputId": "ad6faf65-5361-434d-a46d-a7c176603a14"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71243</th>\n",
              "      <td>То чувство, когда у тебя не работает ВК и ты никак не можешь поздравить остальных своих друзей ;(\\n#НГ  #пздц</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31533</th>\n",
              "      <td>По ТНТ идёт полицейская академия. Ностальгия :(</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81528</th>\n",
              "      <td>Мой телефон реагирует на отмерзшие пальцы в перчатках, правда писать не очень удобно:|</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109614</th>\n",
              "      <td>@_Angel_OF_Lord А ПОЧЕМУ ВОКРУГ СТОЛА?\\nПРОСТО МОЯ БОЛЬНАЯ ФАНТАЗИЯ УЖЕ СТРОИТ НЕ ОЧЕНЬ ПРИЛИЧНЫЕ КАРТИНКИ В УМЕ:DD #жаркийчетверг</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109132</th>\n",
              "      <td>-Вы расстались, из-за чего? \\n-Да если бы я сам это знал(</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                      text     label\n",
              "71243                        То чувство, когда у тебя не работает ВК и ты никак не можешь поздравить остальных своих друзей ;(\\n#НГ  #пздц  negative\n",
              "31533                                                                                      По ТНТ идёт полицейская академия. Ностальгия :(  negative\n",
              "81528                                               Мой телефон реагирует на отмерзшие пальцы в перчатках, правда писать не очень удобно:|  negative\n",
              "109614  @_Angel_OF_Lord А ПОЧЕМУ ВОКРУГ СТОЛА?\\nПРОСТО МОЯ БОЛЬНАЯ ФАНТАЗИЯ УЖЕ СТРОИТ НЕ ОЧЕНЬ ПРИЛИЧНЫЕ КАРТИНКИ В УМЕ:DD #жаркийчетверг  positive\n",
              "109132                                                                           -Вы расстались, из-за чего? \\n-Да если бы я сам это знал(  negative"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXiMonnfbxaT"
      },
      "source": [
        "Воспользуемся функцией для предобработки текста, которую мы написали в прошлом семинаре:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-_mXWfEbxaT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from functools import lru_cache\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "m = MorphAnalyzer()\n",
        "regex = re.compile(\"[А-Яа-яA-z]+\")\n",
        "\n",
        "def words_only(text, regex=regex):\n",
        "    try:\n",
        "        return regex.findall(text.lower())\n",
        "    except:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCfXSovkbxaU"
      },
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=128)\n",
        "def lemmatize_word(token, pymorphy=m):\n",
        "    return pymorphy.parse(token)[0].normal_form\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatize_word(w) for w in text]\n",
        "\n",
        "\n",
        "mystopwords = stopwords.words('russian') \n",
        "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
        "    return [w for w in lemmas if not w in stopwords and len(w) > 3]\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = words_only(text)\n",
        "    lemmas = lemmatize_text(tokens)\n",
        "    \n",
        "    return ' '.join(remove_stopwords(lemmas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyckQ3Z9bxaU",
        "outputId": "cb971e05-8163-4277-85be-a0e0872223da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 226834/226834 [00:54<00:00, 4168.88it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75734</th>\n",
              "      <td>мне мало мало мало мне малооо зарядки на плеере:(</td>\n",
              "      <td>negative</td>\n",
              "      <td>мало мало мало малооо зарядка плеер</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22310</th>\n",
              "      <td>@ehnevermind Я никогда его не найду &amp;gt;;(</td>\n",
              "      <td>negative</td>\n",
              "      <td>ehnevermind найти</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73725</th>\n",
              "      <td>RT @igajaqytmu: смотрит на вас, как на вебкамеру. И странно скалится при этом %))</td>\n",
              "      <td>positive</td>\n",
              "      <td>igajaqytmu смотреть вебкамера странно скалиться</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16467</th>\n",
              "      <td>всё вернулось на свои места))\\nопять в чс\\nя счастлив\\nтварь</td>\n",
              "      <td>positive</td>\n",
              "      <td>вернуться свой место счастливый тварь</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9758</th>\n",
              "      <td>@funkyboyakeem я не пила мандариновую водку) и с возрастом не угадал</td>\n",
              "      <td>positive</td>\n",
              "      <td>funkyboyakeem пила мандариновый водка возраст угадать</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                    text     label                                                 lemmas\n",
              "75734                                  мне мало мало мало мне малооо зарядки на плеере:(  negative                    мало мало мало малооо зарядка плеер\n",
              "22310                                         @ehnevermind Я никогда его не найду &gt;;(  negative                                      ehnevermind найти\n",
              "73725  RT @igajaqytmu: смотрит на вас, как на вебкамеру. И странно скалится при этом %))  positive        igajaqytmu смотреть вебкамера странно скалиться\n",
              "16467                       всё вернулось на свои места))\\nопять в чс\\nя счастлив\\nтварь  positive                  вернуться свой место счастливый тварь\n",
              "9758                @funkyboyakeem я не пила мандариновую водку) и с возрастом не угадал  positive  funkyboyakeem пила мандариновый водка возраст угадать"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "\n",
        "with Pool(4) as p:\n",
        "    lemmas = list(tqdm(p.imap(clean_text, df['text']), total=len(df)))\n",
        "    \n",
        "df['lemmas'] = lemmas\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xALyFcNnbxaU"
      },
      "source": [
        "Разбиваем на train и test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3MD0bex8icC"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.lemmas, df.label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppAtTFc8icE"
      },
      "source": [
        "## Мешок слов (Bag of Words, BoW)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gJLFKQ38icE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMGIJ8C8icH"
      },
      "source": [
        "... Но сперва пару слов об n-граммах. Что такое n-граммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0-y2A6k8icH"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_lgEYzY8icJ",
        "outputId": "f9d482bd-0c45-4830-9e7b-1b49d1290468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Факультет',),\n",
              " ('компьютерных',),\n",
              " ('наук',),\n",
              " ('Высшей',),\n",
              " ('школы',),\n",
              " ('экономики',)]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = 'Факультет компьютерных наук Высшей школы экономики'.split()\n",
        "list(ngrams(sent, 1)) # униграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDRi-68C8icM",
        "outputId": "8d04b779-f404-4c5e-e06c-125ef4f4572d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Факультет', 'компьютерных'),\n",
              " ('компьютерных', 'наук'),\n",
              " ('наук', 'Высшей'),\n",
              " ('Высшей', 'школы'),\n",
              " ('школы', 'экономики')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 2)) # биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaUeKBDh8icO",
        "outputId": "831fbc23-1c08-4b7b-e5fd-856c10132bdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Факультет', 'компьютерных', 'наук'),\n",
              " ('компьютерных', 'наук', 'Высшей'),\n",
              " ('наук', 'Высшей', 'школы'),\n",
              " ('Высшей', 'школы', 'экономики')]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 3)) # триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z325_XfX8icS",
        "outputId": "5da7999f-0cf3-4964-8d23-e958f21d5f43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Факультет', 'компьютерных', 'наук', 'Высшей', 'школы'),\n",
              " ('компьютерных', 'наук', 'Высшей', 'школы', 'экономики')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 5)) # ... пентаграммы?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80h0e8FJ8icV"
      },
      "source": [
        "Итак, мы хотим преобразовать наши обработанные данные в вектора с помощью мешка слов. Мешок слов можно строить как для отдельных слов (лемм в нашем случае), так и для n-грамм, и это может улучшать качество. \n",
        "\n",
        "Объект `CountVectorizer` делает простую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SQaMJbl8icW"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1)) # строим BoW для слов\n",
        "bow = vec.fit_transform(x_train) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6eZOyAf8icY"
      },
      "source": [
        "ngram_range отвечает за то, какие n-граммы мы используем в качестве признаков:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы.\n",
        "\n",
        "В vec.vocabulary_ лежит словарь: соответствие слов и их индексов в словаре:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8cncS9M8icY",
        "outputId": "c1943f5c-3400-4fb1-d218-c8f89d225634"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('жаль', 110128),\n",
              " ('стать', 153946),\n",
              " ('редко', 147080),\n",
              " ('заходить', 112987),\n",
              " ('новогодний', 130623),\n",
              " ('каникулы', 116354),\n",
              " ('пролететь', 144046),\n",
              " ('день', 106675),\n",
              " ('заметить', 112070),\n",
              " ('успеть', 160401)]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(vec.vocabulary_.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phc1fm5lbxaW",
        "outputId": "57c8894f-95eb-404f-c7e0-d19e82a79b0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x169048 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 4 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1flpUdCkbxaW"
      },
      "source": [
        "Теперь у нас есть вектора, на которых можно обучать модели! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Rq60E68ica",
        "outputId": "5caa0b70-3a96-4efb-f241-93120aa9f362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTSQUK60bxaW"
      },
      "source": [
        "Посмотрим на качество классификации на тестовой выборке. Для этого выведем classification_report из модуля [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)\n",
        "\n",
        "В качестве целевой метрики качества будем рассматривать macro average f1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf8gqHSD8icc",
        "outputId": "71a8da2f-d5c6-4070-b28e-d99cc7145832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.74      0.73      0.74     28385\n",
            "    positive       0.73      0.75      0.74     28324\n",
            "\n",
            "    accuracy                           0.74     56709\n",
            "   macro avg       0.74      0.74      0.74     56709\n",
            "weighted avg       0.74      0.74      0.74     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdoG6YCr8icf"
      },
      "source": [
        "Попробуем сделать то же самое для триграмм:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GUDAWcz8icg",
        "outputId": "88bfc3b3-7eb9-4082-f500-84d75c2e3d68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.97      0.53      0.68     51252\n",
            "    positive       0.16      0.84      0.27      5457\n",
            "\n",
            "    accuracy                           0.56     56709\n",
            "   macro avg       0.56      0.69      0.48     56709\n",
            "weighted avg       0.89      0.56      0.64     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(3, 3))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter = 300)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO0bqtgPbxaX"
      },
      "source": [
        "Видим, что качество существенно хуже. Ниже мы поймем, почему это так."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PTszK9h8ick"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ9Td4bw8icm"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений – tf-idf каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "TF (term frequency) – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_t}{\\sum_k n_k} $$\n",
        "\n",
        "`t` -- слово (term), `d` -- документ, $n_t$ -- количество вхождений слова, $n_k$ -- количество вхождений остальных слов\n",
        "\n",
        "IDF (inverse document frequency) – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "`t` -- слово (term), `D` -- коллекция документов\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t,d,D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Ключевая идея этого подхода – если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8B_Q8qP8icm"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn-S--vi8ico",
        "outputId": "42455755-3c0c-4ab2-c08d-caf318cad0d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.70      0.74      0.72     26327\n",
            "    positive       0.77      0.72      0.74     30382\n",
            "\n",
            "    accuracy                           0.73     56709\n",
            "   macro avg       0.73      0.73      0.73     56709\n",
            "weighted avg       0.74      0.73      0.73     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter = 500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXglD7lb8icq"
      },
      "source": [
        "В этот раз получилось хуже, чем с помощью простого CountVectorizer, то есть использование tf-idf не дало улучшений в качестве. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETq8X_Tb8idz"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Иногда в ходе стандартного препроцессинга теряются важные признаки. Посмотрим, что будет если не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNNR90btbxaX",
        "outputId": "f6d15c45-d8e7-4e93-f030-7044dcf69d80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>52766</th>\n",
              "      <td>RT @tataholiday: хочу вернуть старые времена, опять :( \\n#miss</td>\n",
              "      <td>negative</td>\n",
              "      <td>tataholiday хотеть вернуть старое время miss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                 text     label                                        lemmas\n",
              "52766  RT @tataholiday: хочу вернуть старые времена, опять :( \\n#miss  negative  tataholiday хотеть вернуть старое время miss"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8uBG8pcbxaY",
        "outputId": "c6425bfd-5ae9-4ba8-9c58-36ab7626cd8b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>new_lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24694</th>\n",
              "      <td>Q: подпишись\\nпожалуйста*)  A: http://t.co/XKTaDVlUP2</td>\n",
              "      <td>positive</td>\n",
              "      <td>подписаться пожалуйста http xktadvlup</td>\n",
              "      <td>q: подпишись\\nпожалуйста*)  a: http://t.co/xktadvlup2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54857</th>\n",
              "      <td>В свои 11 лет, я встречалась с 18-и летним парнем. Мне так жалко таких парней, их друзья оказывается так издеваются. Мы встречались 2 года((</td>\n",
              "      <td>negative</td>\n",
              "      <td>свой встречаться летний парень жалко парный друг оказываться издеваться встречаться</td>\n",
              "      <td>в свои 11 лет, я встречалась с 18-и летним парнем. мне так жалко таких парней, их друзья оказывается так издеваются. мы встречались 2 года((</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7777</th>\n",
              "      <td>Общажный движ закончился тем что все спят, а я нихуя:)</td>\n",
              "      <td>positive</td>\n",
              "      <td>общажный движ закончиться весь спать нихуй</td>\n",
              "      <td>общажный движ закончился тем что все спят, а я нихуя:)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                               text     label                                                                               lemmas                                                                                                                                    new_lemmas\n",
              "24694                                                                                         Q: подпишись\\nпожалуйста*)  A: http://t.co/XKTaDVlUP2  positive                                                подписаться пожалуйста http xktadvlup                                                                                         q: подпишись\\nпожалуйста*)  a: http://t.co/xktadvlup2\n",
              "54857  В свои 11 лет, я встречалась с 18-и летним парнем. Мне так жалко таких парней, их друзья оказывается так издеваются. Мы встречались 2 года((  negative  свой встречаться летний парень жалко парный друг оказываться издеваться встречаться  в свои 11 лет, я встречалась с 18-и летним парнем. мне так жалко таких парней, их друзья оказывается так издеваются. мы встречались 2 года((\n",
              "7777                                                                                         Общажный движ закончился тем что все спят, а я нихуя:)  positive                                           общажный движ закончиться весь спать нихуй                                                                                        общажный движ закончился тем что все спят, а я нихуя:)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['new_lemmas'] = df.text.apply(lambda x: x.lower())\n",
        "df.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omCZV8WhbxaY"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.new_lemmas, df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue0DUsX18id0",
        "outputId": "f3858c1f-554f-4f18-8cc4-6fa5416702ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27863\n",
            "    positive       1.00      1.00      1.00     28846\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter = 300)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymeD7-B8id3"
      },
      "source": [
        "Как можно видеть, если оставить пунктуацию, то все метрики равны 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzRu93HUbxaY",
        "outputId": "aca2f184-fa85-4139-9889-5fe89cac15c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(266706, 266706)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vec.vocabulary_), len(clf.coef_[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGKuAvYR8id3",
        "outputId": "0f51ba99-7190-4d26-da3b-1816ccaccea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('@', 0.1471878855818156)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "importances = list(zip(vec.vocabulary_, clf.coef_[0]))\n",
        "importances[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a9ZrJ5WbxaY",
        "outputId": "bdf2f84d-f16b-401a-caba-80b873236309"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('что', 58.77436936506602),\n",
              " ('//t.co/cuphpysvcp', 27.295181101473574),\n",
              " ('у', 12.578240551762557),\n",
              " ('daromand77', 10.80497075752631),\n",
              " ('кюхенио', 9.111201609919437),\n",
              " ('учете', 8.102007005355661),\n",
              " ('50', 7.678739507821211),\n",
              " ('него', 5.254932152879905),\n",
              " ('коллега', 4.744012822521124),\n",
              " ('останавливало', 4.679962041879572)]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_importances = sorted(importances, key = lambda x: -x[1])\n",
        "sorted_importances[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QxmioaZ8id8"
      },
      "source": [
        "Посмотрим, как один из наиболее значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17LPHPGR8id9",
        "outputId": "50c87666-43d1-401e-fef9-c19d4faa5e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32901\n",
            "    positive       0.83      1.00      0.90     23808\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.91      0.92      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIFCnn3jbxaZ"
      },
      "source": [
        "Можно видеть, что это уже позволяет достаточно хорошо классифицировать тексты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7VjSCog8id_"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве признаком используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSO-k4wA8id_",
        "outputId": "29b7529b-d86c-466a-802d-186284782503"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aksenov/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.99      1.00      0.99     27773\n",
            "    positive       1.00      0.99      1.00     28936\n",
            "\n",
            "    accuracy                           0.99     56709\n",
            "   macro avg       0.99      0.99      0.99     56709\n",
            "weighted avg       0.99      0.99      0.99     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6JwqL-8ieE"
      },
      "source": [
        "Таким образом, становится понятно, почему на этих данных качество классификации 1. Так или иначе, на символах классифицировать тоже можно.\n",
        "\n",
        "Ещё одна замечательная особенность символьных признаков: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV1B3kBybxaZ"
      },
      "source": [
        "## Итоги\n",
        "\n",
        " На этом занятии мы\n",
        "* познакомились с задачей бинарной классификации текстов.\n",
        "\n",
        "* научились строить простые признаки на основе метода \"мешка слов\" с помощью библиотеки sklearn: CountVectorizer и TfidfVectorizer.\n",
        "\n",
        "* использовали для классификации линейную модель логистической регрессии.\n",
        "\n",
        "* поняли, что многое зависит от подхода к предобработки текста и от признаков, которые используются в модели.\n",
        "\n",
        "* увидели, что в некоторых задачах важно использование каждого символа из текста, в том числе пунктуации.\n",
        "\n",
        "На следующих занятиях мы рассмотрим более сложные модели построения признаков и классификации текстов."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GTJsMuTX8wjh",
        "wZA9awaI8wjy",
        "_f54Z_mabxaP",
        "nADMfeJNbxaS",
        "8ppAtTFc8icE",
        "4PTszK9h8ick",
        "ETq8X_Tb8idz",
        "B7VjSCog8id_",
        "BV1B3kBybxaZ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}